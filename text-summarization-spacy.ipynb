{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  TEXT SUMMARIZATION USING THE FREQUENCY METHOD SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text  = \"\"\"\n",
    "You see the problem with these young children is they do not want to listen now I told Jane\n",
    "to take the trash out but she refused. Now, Ethan and Abby are fighting over a cup of tea. I\n",
    "don't know what to do for them, but probably we shall take them to the daycare, then they\n",
    "can be okay. Also, they will be having exams starting next week. So, they will need to be\n",
    "sleeping early enough so that they can get enough rest for for so that they can perform well\n",
    "in the exam. Other than that, everything at home is okay. How is your trip and the children\n",
    "are expecting some chocolate when you return. So, whatever you do, stock up on chocolate\n",
    "biscuits, sweets, and all these pleasantries that children like they will be waiting by\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from math import ceil\n",
    "#from spacy.lang.en import stop_words\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading spacy model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'You',\n",
       " 'see',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'these',\n",
       " 'young',\n",
       " 'children',\n",
       " 'is',\n",
       " 'they',\n",
       " 'do',\n",
       " 'not',\n",
       " 'want',\n",
       " 'to',\n",
       " 'listen',\n",
       " 'now',\n",
       " 'I',\n",
       " 'told',\n",
       " 'Jane',\n",
       " '\\n',\n",
       " 'to',\n",
       " 'take',\n",
       " 'the',\n",
       " 'trash',\n",
       " 'out',\n",
       " 'but',\n",
       " 'she',\n",
       " 'refused',\n",
       " '.',\n",
       " 'Now',\n",
       " ',',\n",
       " 'Ethan',\n",
       " 'and',\n",
       " 'Abby',\n",
       " 'are',\n",
       " 'fighting',\n",
       " 'over',\n",
       " 'a',\n",
       " 'cup',\n",
       " 'of',\n",
       " 'tea',\n",
       " '.',\n",
       " 'I',\n",
       " '\\n',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'for',\n",
       " 'them',\n",
       " ',',\n",
       " 'but',\n",
       " 'probably',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'take',\n",
       " 'them',\n",
       " 'to',\n",
       " 'the',\n",
       " 'daycare',\n",
       " ',',\n",
       " 'then',\n",
       " 'they',\n",
       " '\\n',\n",
       " 'can',\n",
       " 'be',\n",
       " 'okay',\n",
       " '.',\n",
       " 'Also',\n",
       " ',',\n",
       " 'they',\n",
       " 'will',\n",
       " 'be',\n",
       " 'having',\n",
       " 'exams',\n",
       " 'starting',\n",
       " 'next',\n",
       " 'week',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'they',\n",
       " 'will',\n",
       " 'need',\n",
       " 'to',\n",
       " 'be',\n",
       " '\\n',\n",
       " 'sleeping',\n",
       " 'early',\n",
       " 'enough',\n",
       " 'so',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'get',\n",
       " 'enough',\n",
       " 'rest',\n",
       " 'for',\n",
       " 'for',\n",
       " 'so',\n",
       " 'that',\n",
       " 'they',\n",
       " 'can',\n",
       " 'perform',\n",
       " 'well',\n",
       " '\\n',\n",
       " 'in',\n",
       " 'the',\n",
       " 'exam',\n",
       " '.',\n",
       " 'Other',\n",
       " 'than',\n",
       " 'that',\n",
       " ',',\n",
       " 'everything',\n",
       " 'at',\n",
       " 'home',\n",
       " 'is',\n",
       " 'okay',\n",
       " '.',\n",
       " 'How',\n",
       " 'is',\n",
       " 'your',\n",
       " 'trip',\n",
       " 'and',\n",
       " 'the',\n",
       " 'children',\n",
       " '\\n',\n",
       " 'are',\n",
       " 'expecting',\n",
       " 'some',\n",
       " 'chocolate',\n",
       " 'when',\n",
       " 'you',\n",
       " 'return',\n",
       " '.',\n",
       " 'So',\n",
       " ',',\n",
       " 'whatever',\n",
       " 'you',\n",
       " 'do',\n",
       " ',',\n",
       " 'stock',\n",
       " 'up',\n",
       " 'on',\n",
       " 'chocolate',\n",
       " '\\n',\n",
       " 'biscuits',\n",
       " ',',\n",
       " 'sweets',\n",
       " ',',\n",
       " 'and',\n",
       " 'all',\n",
       " 'these',\n",
       " 'pleasantries',\n",
       " 'that',\n",
       " 'children',\n",
       " 'like',\n",
       " 'they',\n",
       " 'will',\n",
       " 'be',\n",
       " 'waiting',\n",
       " 'by',\n",
       " '\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = punctuation + '\\n'\n",
    "punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a word frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stop_words:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word frequncy dictionary contains word with corresponding frequency od occurence in the input text. Also notice that the stop words have been removed along with punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 1, 'young': 1, 'children': 3, 'want': 1, 'listen': 1, 'told': 1, 'Jane': 1, 'trash': 1, 'refused': 1, 'Ethan': 1, 'Abby': 1, 'fighting': 1, 'cup': 1, 'tea': 1, 'know': 1, 'probably': 1, 'shall': 1, 'daycare': 1, 'okay': 2, 'having': 1, 'exams': 1, 'starting': 1, 'week': 1, 'need': 1, 'sleeping': 1, 'early': 1, 'rest': 1, 'perform': 1, 'exam': 1, 'home': 1, 'trip': 1, 'expecting': 1, 'chocolate': 2, 'return': 1, 'stock': 1, 'biscuits': 1, 'sweets': 1, 'pleasantries': 1, 'like': 1, 'waiting': 1}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize word frequency dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_frequency = max(word_frequencies.values())\n",
    "max_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide all the frequencies by the max frequency to obtain normalized frequency for all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = word_frequencies[word]/max_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problem': 0.3333333333333333, 'young': 0.3333333333333333, 'children': 1.0, 'want': 0.3333333333333333, 'listen': 0.3333333333333333, 'told': 0.3333333333333333, 'Jane': 0.3333333333333333, 'trash': 0.3333333333333333, 'refused': 0.3333333333333333, 'Ethan': 0.3333333333333333, 'Abby': 0.3333333333333333, 'fighting': 0.3333333333333333, 'cup': 0.3333333333333333, 'tea': 0.3333333333333333, 'know': 0.3333333333333333, 'probably': 0.3333333333333333, 'shall': 0.3333333333333333, 'daycare': 0.3333333333333333, 'okay': 0.6666666666666666, 'having': 0.3333333333333333, 'exams': 0.3333333333333333, 'starting': 0.3333333333333333, 'week': 0.3333333333333333, 'need': 0.3333333333333333, 'sleeping': 0.3333333333333333, 'early': 0.3333333333333333, 'rest': 0.3333333333333333, 'perform': 0.3333333333333333, 'exam': 0.3333333333333333, 'home': 0.3333333333333333, 'trip': 0.3333333333333333, 'expecting': 0.3333333333333333, 'chocolate': 0.6666666666666666, 'return': 0.3333333333333333, 'stock': 0.3333333333333333, 'biscuits': 0.3333333333333333, 'sweets': 0.3333333333333333, 'pleasantries': 0.3333333333333333, 'like': 0.3333333333333333, 'waiting': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", You see the problem with these young children is they do not want to listen now I told Jane\n",
      "to take the trash out, but she refused., Now, Ethan and Abby are fighting over a cup of tea., I\n",
      "don't know what to do for them, but probably we shall take them to the daycare, then they\n",
      "can be okay., Also, they will be having exams starting next week., So, they will need to be\n",
      "sleeping early enough so that they can get enough rest for for so that they can perform well\n",
      "in the exam., Other than that, everything at home is okay., How is your trip and the children\n",
      "are expecting some chocolate when you return., So, whatever you do, stock up on chocolate\n",
      "biscuits, sweets, and all these pleasantries that children like they will be waiting by\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentence_tokens = [sent for sent in doc.sents]\n",
    "print(sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sentence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "senetence_scores = {}\n",
    "for sent in sentence_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_frequencies.keys():\n",
    "            if sent not in senetence_scores.keys():\n",
    "                senetence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "            else:\n",
    "                senetence_scores[sent] += word_frequencies[word.text.lower()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{You see the problem with these young children is they do not want to listen now I told Jane\n",
       " to take the trash out: 3.0,\n",
       " but she refused.: 0.3333333333333333,\n",
       " Now, Ethan and Abby are fighting over a cup of tea.: 1.0,\n",
       " I\n",
       " don't know what to do for them, but probably we shall take them to the daycare, then they\n",
       " can be okay.: 2.0,\n",
       " Also, they will be having exams starting next week.: 1.3333333333333333,\n",
       " So, they will need to be\n",
       " sleeping early enough so that they can get enough rest for for so that they can perform well\n",
       " in the exam.: 1.9999999999999998,\n",
       " Other than that, everything at home is okay.: 1.0,\n",
       " How is your trip and the children\n",
       " are expecting some chocolate when you return.: 2.6666666666666665,\n",
       " So, whatever you do, stock up on chocolate\n",
       " biscuits, sweets, and all these pleasantries that children like they will be waiting by: 3.666666666666667}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senetence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top sentences with maximum scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_length = ceil((len(sentence_tokens)*0.3))\n",
    "select_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = nlargest(select_length,senetence_scores,key=senetence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[So, whatever you do, stock up on chocolate\n",
       " biscuits, sweets, and all these pleasantries that children like they will be waiting by,\n",
       " You see the problem with these young children is they do not want to listen now I told Jane\n",
       " to take the trash out,\n",
       " How is your trip and the children\n",
       " are expecting some chocolate when you return.]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So, whatever you do, stock up on chocolate\\nbiscuits, sweets, and all these pleasantries that children like they will be waiting by\\n', 'You see the problem with these young children is they do not want to listen now I told Jane\\nto take the trash out', 'How is your trip and the children\\nare expecting some chocolate when you return.']\n"
     ]
    }
   ],
   "source": [
    "final_summary = [word.text for word in summary]\n",
    "\n",
    "print(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a468afd87b214c8448be3cc243daf44d9fe1315ae30f84a6ae7ec98f1fe5184"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
